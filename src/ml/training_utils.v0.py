'''
this module defines utilities to construct, train, and evaluate machine-learning classification models
using support vector machines (svm), xgboost classifiers, and multilayer perceptrons (mlp). it represents
the second step of the workflow, following data loading and preprocessing.

the module expects a pandas dataframe containing feature columns and a binary 'disease_status' label.
it returns trained model objects, predictions, and class probabilities.

functions
---------
train_model
    train a model using gridsearchcv with a configurable train/test split.
evaluate_model
    evaluate a trained model on a held-out test set and compute predictions and probabilities.
train_evaluate_model
    convenience function that trains and evaluates a model in a single call.

parameters
----------
input dataframe : pandas.dataframe
    dataframe containing numeric feature columns and a 'disease_status' label.
model type : {'svm', 'xgboost', 'mlp', 'all'}
    identifier of which model(s) to define, train, and evaluate.

returns
-------
trained model(s) : gridsearchcv or dict of gridsearchcv
    the fitted estimator(s) selected by grid search.
y_test : array or dict of arrays
    ground-truth labels of the held-out test set.
y_pred : array or dict of arrays
    predicted class labels generated by the trained model(s).
y_proba : array or dict of arrays
    predicted probabilities for the positive class.

examples
--------
train and evaluate a single model:
>>> model, y_test, y_pred, y_proba = train_evaluate_model(df, 'svm')

train all models at once:
>>> models, y_test, y_pred, y_proba = train_evaluate_model(df, 'all')

train and evaluate separately:
>>> model = train_model(df, 'svm')
>>> y_test, y_pred, y_proba = evaluate_model(model, df)

hyperparameters, cross-validation settings, random state, and scoring can be customized:
>>> model = train_model(
...     df,
...     'svm',
...     hyperparameters={'C': [0.1, 1, 10]},
...     split_ratio=0.3,
...     random_state=123,
...     kfold=5
... )
'''

# -- temp todo list --

# - [x] make a pretty print function for the hyperparameters dictionary and the best model
# - [x] GLOBAL variables to be added instead of hardcoded values
# - [x] add docstrings to functions
# - [x] make some functions private if not needed to be used outside the module
# - [ ] make it take json or yaml hyperparameter inputs and parse them to proper types
# - [ ] add option to save trained model to file
# - [ ] logging
# - [ ] make the flobal variable for hyperparameters one HYPERPARAMTERS that is a dict of keys the different models: better for generaliation


import sklearn
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
import xgboost
import numpy as np
import pandas as pd
import joblib

import warnings
from sklearn.exceptions import ConvergenceWarning
warnings.filterwarnings("ignore", category=ConvergenceWarning)

# -- GLOBAL CONSTANTS --
DEFAULT_KFOLD = 3
DEFAULT_SPLIT_RATIO = 0.2
DEFAULT_RANDOM_STATE = 42
DEFAULT_SCORING = "accuracy"
DEFAULT_SAVE= False
CACHE_DIR='.cache/'
SYSOUT_FILE="training_utils.log" 

# -------------------------- SETTERS FOR GLOBALS --------------------------

# -- setters for caching
import os
def set_cache_dir(path: str):
    """
    set the cache directory for logging

    parameters
    ----------
    path : str
        path to cache directory.
    """
    global CACHE_DIR
    CACHE_DIR = path
    if os.path.exists(CACHE_DIR) is False:
        os.makedirs(CACHE_DIR)
def set_sysout_file(filename: str):
    """
    set the sysout log filename

    parameters
    ----------
    filename : str
        log filename.
    """
    global SYSOUT_FILE
    SYSOUT_FILE = filename
# -- setter for saving trained models
def to_save_model(save: bool):
    """
    set whether to save trained models
    parameters
    ----------
    save : bool
        whether to save trained models.
    """
    global DEFAULT_SAVE
    DEFAULT_SAVE = save
set_cache_dir(CACHE_DIR) # -- running to create .cache/ if not existing

def set_kfold(kfold: int):
    """
    set the default number of folds for cross-validation

    parameters
    ----------
    kfold : int
    """
    global DEFAULT_KFOLD
    DEFAULT_KFOLD = kfold
def set_split_ratio(ratio: float):
    """
    set the default split ratio for train/test split

    parameters
    ----------
    ratio : float
        proportion of data used for testing.
    """
    global DEFAULT_SPLIT_RATIO
    DEFAULT_SPLIT_RATIO = ratio
def set_random_state(state: int):
    """
    set the default random state for reproducibility

    parameters
    ----------
    state : int
        random seed.
    """
    global DEFAULT_RANDOM_STATE
    DEFAULT_RANDOM_STATE = state

def set_grid_search_scoring(scoring: str):
    """
    set the default scoring metric for grid search

    parameters
    ----------
    scoring : str
        scoring metric.
    """
    global DEFAULT_SCORING
    DEFAULT_SCORING = scoring

import sys
class Logger:
    def __init__(self, filename):
        self.terminal = sys.stdout  
        self.log = open(filename, "w")  
    def write(self, message):
        self.terminal.write(message) 
        self.log.write(message)    
    def flush(self):
        self.terminal.flush()
        self.log.flush()

sys.stdout = Logger(CACHE_DIR+SYSOUT_FILE)

import pprint
pp = pprint.PrettyPrinter(indent=4)

# -- MODEL REGISTRY --
MODELS = {'svm': SVC(), 'xgboost': xgboost.XGBClassifier(), 'mlp': MLPClassifier()}

# -- DEFAULT HYPERPARAMETER GRIDS --
SVM_HYPERPARAMS = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf'], 'gamma': ['scale', 'auto']}
XGBOOST_HYPERPARAMS = {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 7], 'learning_rate': [0.01, 0.1, 0.2]}
MLP_HYPERPARAMS = {'hidden_layer_sizes': [(50,), (100,), (50, 50)], 'activation': ['relu', 'tanh'],'solver': ['adam', 'sgd'], 'learning_rate_init': [0.001, 0.01, 0.1]}

def set_svm_hyperparameters(hyperparameters: dict):
    """
    set default hyperparameters for svm

    parameters
    ----------
    hyperparameters : dict
        hyperparameter grid.
    """
    _validate_hyperparameters('svm', hyperparameters)
    global SVM_HYPERPARAMS
    SVM_HYPERPARAMS = hyperparameters

def set_xgboost_hyperparameters(hyperparameters: dict):
    """
    set default hyperparameters for xgboost

    parameters
    ----------
    hyperparameters : dict
        hyperparameter grid.
    """
    _validate_hyperparameters('xgboost', hyperparameters)
    global XGBOOST_HYPERPARAMS
    XGBOOST_HYPERPARAMS = hyperparameters

def set_mlp_hyperparameters(hyperparameters: dict):
    """
    set default hyperparameters for mlp

    parameters
    ----------
    hyperparameters : dict
        hyperparameter grid.
    """
    _validate_hyperparameters('mlp', hyperparameters)
    global MLP_HYPERPARAMS
    MLP_HYPERPARAMS = hyperparameters

# -- one setter for all globals:
def set_global_variable(var_name: str, value):
    """
    set a global variable by name

    parameters
    ----------
    var_name : str
        name of the global variable to set
    value : any
        value to assign to the global variable

    Options
    -------
    'CACHE_DIR' : str
        path to cache directory (saves logs and models if set to true)
    'SYSOUT_FILE' : str
        filename for system output log
    'DEFAULT_SAVE' : bool
        whether to save trained models by default
    'DEFAULT_KFOLD' : int
        number of folds for cross-validation
    'DEFAULT_SPLIT_RATIO' : float
        proportion of data used for testing
    'DEFAULT_RANDOM_STATE' : int
        random seed for reproducibility
    'DEFAULT_SCORING' : str
        scoring metric for grid search
    'SVM_HYPERPARAMS' : dict
        hyperparameter grid for svm
    'XGBOOST_HYPERPARAMS' : dict
        hyperparameter grid for xgboost
    'MLP_HYPERPARAMS' : dict
        hyperparameter grid for mlp

    raises
    ------
    ValueError
        if the variable name is not recognized
    """
    globals_setters_dict = {
        'CACHE_DIR': set_cache_dir,
        'SYSOUT_FILE': set_sysout_file,
        'DEFAULT_SAVE': to_save_model,
        'DEFAULT_KFOLD': set_kfold,
        'DEFAULT_SPLIT_RATIO': set_split_ratio,
        'DEFAULT_RANDOM_STATE': set_random_state,
        'DEFAULT_SCORING': set_grid_search_scoring,
        'SVM_HYPERPARAMS': set_svm_hyperparameters,
        'XGBOOST_HYPERPARAMS': set_xgboost_hyperparameters,
        'MLP_HYPERPARAMS': set_mlp_hyperparameters
    }
    if var_name in globals_setters_dict:
        # -- run setter function
        globals_setters_dict[var_name](value)
    else:
        raise ValueError(f"-- global variable '{var_name}' is not recognized. --")

# -- PRIVATE HELPERS --
def _pretty_print_dict(title: str, d: dict):
    """
    pretty print a dictionary with a title

    parameters
    ----------
    title : str
        title printed above the dictionary
    d : dict
        dictionary to print
    """
    print(f"\n-- {title} --")
    pp.pprint(d)
    print("\n")

def _pretty_print_gridsearchcv(title: str, grid_search: GridSearchCV):
    """
    pretty print the best parameters of a gridsearchcv object with a title

    parameters
    ----------
    title : str
        title printed above the best parameters.
    grid_search : GridSearchCV
        gridsearchcv object.
    """
    print(f"\n-- {title} --")
    pp.pprint(grid_search.best_params_)
    print("\n")

def _validate_hyperparameters(model_type: str, hyperparameters: dict):
    """
    validate that provided hyperparameters exist for the requested model type

    parameters
    ----------
    model_type : str
        model identifier ('svm', 'xgboost', 'mlp').
    hyperparameters : dict
        hyperparameter dictionary to validate.

    raises
    ------
    valueerror
        if the model type or any hyperparameter key is invalid.
    """
    model_type = model_type.lower()
    if model_type not in MODELS:
        raise ValueError(f"-- model type '{model_type}' is not supported: choose from {list(MODELS.keys())} --")
    valid_params = MODELS[model_type].get_params().keys()
    for param in hyperparameters.keys():
        if param not in valid_params:
            raise ValueError(f"-- hyperparameter '{param}' is invalid for '{model_type}'. valid keys: {list(valid_params)} --")


def _json_to_hyperparameters(json_dict: dict) -> dict:
    """
    convert json-like string values into python-compatible types

    parameters
    ----------
    json_dict : dict
        dictionary where some values may be string-formatted lists.

    returns
    -------
    dict
        parsed hyperparameter dictionary.
    """
    hyperparameters = {}
    for key, value in json_dict.items():
        if isinstance(value, str) and value.startswith("[") and value.endswith("]"):
            value = value.strip("[]").split(",")
            value = [v.strip().strip("'").strip('"') for v in value]
        hyperparameters[key] = value
    return hyperparameters


# -- MODEL DEFINITIONS --
def _define_svm(grid_params=None, kfold=DEFAULT_KFOLD):
    """
    construct a gridsearchcv object for an svm classifier

    parameters
    ----------
    grid_params : dict, optional
        hyperparameter grid for svm.
    kfold : int, optional
        number of cross-validation folds.

    returns
    -------
    GridSearchCV
        configured gridsearchcv object for SVC.
    """
    if grid_params:
        _validate_hyperparameters("svm", grid_params)
    else:
        grid_params = SVM_HYPERPARAMS

    _pretty_print_dict("SVM Hyperparameters", grid_params)
    return GridSearchCV(estimator=SVC(probability=True), param_grid=grid_params,
                        scoring=DEFAULT_SCORING, cv=kfold, n_jobs=-1)


def _define_xgboost(grid_params=None, kfold=DEFAULT_KFOLD):
    """
    construct a gridsearchcv object for an xgboost classifier

    parameters
    ----------
    grid_params : dict, optional
        hyperparameter grid.
    kfold : int, optional
        number of cross-validation folds.

    returns
    -------
    GridSearchCV
        configured gridsearchcv object for XGBClassifier.
    """
    if grid_params:
        _validate_hyperparameters("xgboost", grid_params)
    else:
        grid_params = XGBOOST_HYPERPARAMS

    _pretty_print_dict("XGBoost Hyperparameters", grid_params)
    return GridSearchCV(estimator=xgboost.XGBClassifier(eval_metric="logloss"),
                        param_grid=grid_params, scoring=DEFAULT_SCORING, cv=kfold, n_jobs=-1)


def _define_mlp(grid_params=None, kfold=DEFAULT_KFOLD):
    """
    construct a gridsearchcv object for an mlp classifier

    parameters
    ----------
    grid_params : dict, optional
        hyperparameter grid.
    kfold : int, optional
        number of folds for cross-validation.

    returns
    -------
    GridSearchCV
        configured gridsearchcv object for MLPClassifier.
    """
    if grid_params:
        _validate_hyperparameters("mlp", grid_params)
    else:
        grid_params = MLP_HYPERPARAMS
    
    _pretty_print_dict("MLP Hyperparameters", grid_params)
    return GridSearchCV(estimator=MLPClassifier(max_iter=500), param_grid=grid_params,
                        scoring=DEFAULT_SCORING, cv=kfold, n_jobs=-1)


def define_model(model_type: str, hyperparameters: dict = None):
    """
    define and configure a model wrapped in gridsearchcv

    parameters
    ----------
    model_type : str
        model identifier ('svm', 'xgboost', 'mlp').
    hyperparameters : dict, optional
        hyperparameter grid.

    returns
    -------
    GridSearchCV
        configured gridsearchcv instance.
    """
    model_type = model_type.lower()
    if model_type not in MODELS:
        raise ValueError(f"-- model type '{model_type}' is not supported: choose from {list(MODELS.keys())} --")

    # _pretty_print_dict("hyperparameters", hyperparameters)
    if model_type == 'svm':
        return _define_svm(hyperparameters)
    if model_type == 'xgboost':
        return _define_xgboost(hyperparameters)
    if model_type == 'mlp':
        return _define_mlp(hyperparameters)


# -- TRAIN / EVALUATION --
def train_model(df: pd.DataFrame, model_type: str, dataset_name:str, hyperparameters: dict = None,
                split_ratio: float = DEFAULT_SPLIT_RATIO, random_state: int = DEFAULT_RANDOM_STATE,
                save_model:bool=DEFAULT_SAVE):
    """
    train a model using gridsearchcv and a train/test split

    parameters
    ----------
    df : pandas.DataFrame
        input dataframe containing features and 'disease_status'.
    model_type : str
        model identifier ('svm', 'xgboost', 'mlp', 'all')
    dataset_name : str
        name of the dataset being used for saving purposes (gene_expression, RGCN_sample_embeddings, etc.)
    hyperparameters : dict, optional
        hyperparameter grid.
    split_ratio : float, optional
        proportion of data used for testing.
    random_state : int, optional
        seed for reproducibility.
    save_model : bool, optional default to global DEFAULT_SAVE (which is false if not set by to_save_model())
        whether to save the trained model.

    returns
    -------
    GridSearchCV or dict
        trained model or dictionary of trained models.
    """
    print("\n" + "-"*80)
    print(f"TRAINING {model_type.upper()} MODEL")
    print("-"*80)

    y = df["disease_status"].astype(int).values
    X = df.drop(columns=["disease_status"]).values
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=split_ratio, random_state=random_state
    )
    print(f"-- split of {split_ratio} for test set with random state {random_state}")

    if model_type == "all":
        models = {}
        for i, name in enumerate(MODELS.keys()):
            print(f"\n-- training model {name} ({i + 1}/{len(MODELS)})",end="")
            models[name] = train_model(df, name, dataset_name, hyperparameters[name] if hyperparameters else None, split_ratio, random_state, save_model=save_model)
        return models

    print(f"-- model defined {model_type} with:")
    if hyperparameters:
        print("   provided hyperparameters")
    else:
        print("   default hyperparameters for grid search")
    model = define_model(model_type, hyperparameters)

    model.fit(X_train, y_train)
    print(f"-- {model_type.upper()} fitted on training set")
    print(f"-- best parameters found:")
    _pretty_print_dict("best parameters", model.best_params_)

    # -- save model if path provided
    if save_model:
        filepath = os.path.join(CACHE_DIR, f"{dataset_name}_{model_type}_gridsearch_model.joblib")
        joblib.dump(model, filepath)
        print(f"-- trained model saved to: {filepath}")

    return model


def evaluate_model(grid_search: GridSearchCV, df: pd.DataFrame,
                   split_ratio: float = DEFAULT_SPLIT_RATIO, random_state: int = DEFAULT_RANDOM_STATE):
    """
    evaluate a trained model on a test split

    parameters
    ----------
    grid_search : GridSearchCV or dict
        trained model.
    df : pandas.DataFrame
        input dataset.
    split_ratio : float, optional
        proportion used for test set.
    random_state : int, optional
        seed for reproducible splitting.

    returns
    -------
    tuple or dict
        (y_test, y_pred, y_proba) for single model, else dict of results.
    """
    y = df["disease_status"].astype(int).values
    X = df.drop(columns=["disease_status"]).values
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=split_ratio, random_state=random_state
    )

    if type(grid_search) is dict:
        Y_test, Y_pred, Y_proba = {}, {}, {}
        for name, model in grid_search.items():

            print(f"\n-- Evaluating model: {name} --")
            _pretty_print_dict("best parameters", model.best_params_)

            y_pred = model.predict(X_test)
            y_proba = model.predict_proba(X_test)[:, 1]

            print("-- predictions made on test set")

            Y_test[name] = y_test
            Y_pred[name] = y_pred
            Y_proba[name] = y_proba
        return Y_test, Y_pred, Y_proba

    print("\n" + "-"*80)
    print(f"EVALUATING MODEL {type(grid_search.estimator).__name__.upper()}")
    print("-"*80)

    print(f'-- on split ratio {split_ratio} with random state {random_state}')

    best_model = grid_search.best_estimator_
    print("-- best model from grid search:")
    _pretty_print_dict("best parameters", grid_search.best_params_)

    y_pred = best_model.predict(X_test)
    y_proba = best_model.predict_proba(X_test)[:, 1]

    print("-- predictions made on test set")
    print("-"*80+"\n")

    return y_test, y_pred, y_proba


def train_evaluate_model(df: pd.DataFrame, model_type: str, dataset_name: str, hyperparameters: dict = None,
                         split_ratio: float = DEFAULT_SPLIT_RATIO, random_state: int = DEFAULT_RANDOM_STATE, save_model:bool=DEFAULT_SAVE):
    """
    train a model and then evaluate it

    parameters
    ----------
    df : pandas.DataFrame
        input dataframe.
    model_type : str
        model identifier ('svm', 'xgboost', 'mlp', 'all').
    hyperparameters : dict, optional
        hyperparameter grid.
    split_ratio : float, optional
        proportion for test set.
    random_state : int, optional
        seed for reproducibility.

    returns
    -------
    tuple
        (model, y_test, y_pred, y_proba).
    """
    model = train_model(df, model_type, dataset_name=dataset_name, hyperparameters=hyperparameters, split_ratio=split_ratio, random_state=random_state,save_model=save_model)
    y_test, y_pred, y_proba = evaluate_model(model, df, split_ratio, random_state)
    return model, y_test, y_pred, y_proba


# -- arguments to specify the model type from terminal (but would also need to allow df sharing in load_matrix - not essentail now)